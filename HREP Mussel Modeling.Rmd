--- 
title: "HREP Mussel Modeling"
subtitle: "Modeling Mussel Habitat Suitability for Upper Mississippi River Restoration Projects"
date: "`r Sys.Date()`"
author: 
  - Michael Dougherty, Geographer, US Army Corps of Engineers, Rock Island District
  - Davi Michl, Biologist, US Army Corps of Engineers, Rock Island District
  - Dan Kelner, Fisheries Biologist, US Army Corps of Engineers, St. Paul District
site: bookdown::bookdown_site
documentclass: book
bibliography: [mussel_model.bib, packages.bib]
biblio-style: apalike
link-citations: yes
description: "This document describes the process for modeling mussel suitability on Upper Mississippi River Restoration (UMRR) Habitat Rehabilitation and Restoration Projects (HREP)."
---

# Preface {-}
This project has been funded by the US Army Corps of Engineers (USACE) Upper Mississippi River Restoration Program (UMRR). 

```{r echo=FALSE}
knitr::include_graphics("docs/images/HDQLO-03_h120.jpg")
knitr::include_graphics("docs/images/UMRRlogo_tag_rgb_300px.jpg")
```

## Abstract



```{r include=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(.packages(), 'bookdown', 'knitr', 'rmarkdown'),                              'packages.bib')
```


<!--chapter:end:index.Rmd-->

# Introduction

Species Distribution Modeling (SDM) has become a common method for mapping suitable habitats for many species or taxa groups [@elith2008working, @elith2009species]. The `dismo` R package has been created to support SDM in R [see the `dismo` package vignette `sdm`, @hijmans2013species]. Another good overview of SDM in R [@hijmans_sdm].

## Literature Review

* MCAT
* UMESC Pool 8 mussel model
* UMESC Pool 18 mussel model

## Identify Available Datasets
The USACE Mussel Database
HREP pre-project survey

* 2018 Steamboat survey
* 2014 Cordova survey
* 2010 Cordova survey


<!--chapter:end:01-Intro.Rmd-->

# Mussel Data Processing

## Prepare Mussel Sample Data
Working geodatabase: `//mvrdfs/egis/Work/EMP/HREP_Projects/SteamboatSlough/Mussels/SteamboatMusselModel/Data/SB_Mussels.gdb`

* Import sample location point feature class for each survey.
* Import "individual" data tables for each survey.
* Add `double` fields 'Easting' and 'Northing' and calculate geometry for each sample location point feature class.
* Add `double` fields 'Easting' and 'Northing' to each mussel tabular data. 
* Join the sample location point feature class to each mussel tabular data. Directionality of join is important: right-click mussel tabular data and select join, using SAMPLEID as join field.
* Field calculate 'Easting' and 'Northing' from the sample location point feature class. Then remove the join. 
* Use the "Display XY" tool to create events from the mussel tabular data. 
* Export the events to the geodatabase as: `Cordova_YEAR`
* Create 3 new fields (type: long integer) titled 'NumberLive', 'NUM5YEAR', and 'NUM30MM'
* Use field calculator to populate each field with '0' indicating absence and '1' for presence in each new field (number live, number less than 5 years, and number less than 30mm in length).
* Create a definition query to select only those records where: `NUMBER_LIV > 0`
* Dissolve the `Cordova_YEAR` feature class by `SAMPLEID` to collapse mussel tabular data to the unique sample location points. The dissolve tool can also be used to collapse mussel tabular data to unique sample location using multiple fields; e.g., if a count by species is required for analysis, select `SAMPLEID` and `ScientificName`. In the statistics box, add the newly created fields 'Number_Live', 'NUM5YEAR', and 'NUM30MM' and select 'SUM' as statistic to be calculated for each.
* Append all three datasets into one feature class in the .gdb called `Cordova`.  Right-click this feature class and select 'load'. 
* Export to `.csv`. 






<!--chapter:end:02-MusselPreProcessing.Rmd-->

# Calculate Mussel Metrics

## Prepare Mussel Sample Metrics
The purpose of this section is to calculate various metrics of habitat quality for each sampled site. Each of these metrics will be used as presence data as input to create a set of Maxent habitat suitability models. The metrics calculated in this section were taken from the [Upper Mississippi River Mussel Community Assessment Tool (MCAT)](https://acwi.gov/monitoring/conference/2014/2ConcurrentSessions/J2/J2Dunn.pdf) [@dunn2016].

## MCAT Metrics
The following MCAT metrics were selected for inclusion in the model:

* `percent listed species`: percent of listed threatened or endangered species (federal or bordering states) is a measure of sensitive species
* `percent tolerant`: percent of tolerant species (Amblema plicata, Quadrula quadrula, and Obliquaria reflexa) is a measure of a highly disturbed mussel assemblage; i.e., dominated by species tolerant of unstable substrates, silt accumulation, low current velocities, and fluctuating flow conditions.
* `percent tribe Lampsilini`: percent of assemblage that falls within tribe Lampsilini is a measure of species composition, life history, and behavioral characteristics
* `percent juveniles`: percent of mussels <= 5 years-old is a measure of recruitment into an assemblage over the past five years
* `percent >= 15 yrs`: percent of mussels >= 15 years-old is a measure of age distribution in an assemblage
* `Q75 abundance`: a measure of abundance at the 75th quartile calculated as the density (no./m^2)
* `species evenness`: species evenness represents the dominance of an assemblage by a few species using Pielou's evenness index (range 0 to 1)
* `tribe evenness`: tribe evenness represents the dominance of a particular taxonomic group using Pielou's evenness index (range 0 to 1)
* `ES_100`: the expected number of species with a sample size of 100 estimated by rarefaction is a measure of a healthy mussel assemblage


```{r message=FALSE, warning=FALSE, include=FALSE}
# Load libraries
library(sp)
library(labdsv)
library(vegan)
library(ggplot2)
library(dplyr)
library(knitr)
library(kableExtra)
library(arcgisbinding)

# Check for ArcGIS license
arc.check_product()

# Define some ArcGIS helper functions

# Converts ArcGIS feature class to an sp object
arc2sp <- function(fc_path) {
  # Open a connection to the specified ArcGIS feature class
  arcobj <- arcgisbinding::arc.open(fc_path)
  # Select the ArcGIS data
  arc <- arcgisbinding::arc.select(arcobj)
  # Convert the ArcGIS format to the sp format
  sp <- arcgisbinding::arc.data2sp(arc)
}

# Converts an sp object to an ArcGIS feature class
sp2arc <- function(sp_object, fc_path) {
  # Convert the sp object to an ArcGIS data object
  arcobj <- arcgisbinding::arc.sp2data(sp_object)
  # Write the ArcGIS object to a geodatabase feature class
  arcgisbinding::arc.write(data = arcobj, path = fc_path)
}
```

## Create the Individuals table
The purpose of this step is to assemble the "individual" data for each of the surveys within the study area that will be used in the study. 

* Working geodatabase: `//mvrdfs/EGIS/Work/EMP/HREP_Projects/SteamboatSlough/Mussels/SteamboatMusselModel/Data/SB_Mussels.gdb/SB_ILStatePlaneW`
* Create a feature class respresenting all individual mussel samples in the study area, using a definition query to include only live individuals and name it `Cordova_all_individuals`. Note: the Q75 abundance metric will be based on live mussel counts for each quadrat.
* Add the following fields (Type=long integer) to the attribute table: `Listed`, `Tolerant`, `Lampsilini`, `Juveniles`, `Over_15yrs`, `tribe` 

## Create Percent Listed
The purpose of this step is to calculate the percent listed MCAT metric for each sampled site. 

* Dissolve this feature class by `SAMPLE_ID` and `ENAME` (Scientific name).  Add 'SUM_NumberLive' and 'SUM_Listed' fields in the Statistics section of the tool. Uncheck create multipart features. Set Output Feature Class to the working .gdb and name it `PercentListed`.
* Open the `PercentListed` attribute table. Add a field titled 'Perc_listed' Type=double and enter the expression `([SUM_Listed] / [SUM_NumberLive]) * 100` into the field calculator to calculate % of individual listed species per quadrat sample.
* Add 3 fields titled 'species' Type=string and Length=20; 'longitude' Type=double; and 'latitude' Type=double; in that order. 
* Use field calculator to populate the field with species="perc_listed". Use Calculate Geometry to populate the 'longitude' and 'latitude' fields, ensuring that the coordinate system is set to 'NAD 1983 2011 US Feet' and 'StatePlane Illinois West FIPS 1202' and the units are set to US feet.
* Use the Symbology tab to select a suitability threshold based on quartiles (e.g., only selecting "fair" or "good" locations with >=50% listed species). In the Symbology tab, Show: Quantities; field value 'perc_listed'; classes=5; and method=quantile. Export selected to .csv for Maxent model run.

## Create Percent Tolerant
The purpose of this step is to calculate the percent tolerant MCAT metric for each sampled site. 

## Create Percent Tribe Lampsilini
The purpose of this step is to calculate the percent tribe Lampsilini MCAT metric for each sampled site.

## Create Percent Juveniles
The purpose of this step is to calculate the percent juveniles MCAT metric for each sampled site.

## Create Percent Over 15 Years
The purpose of this step is to calculate the percent over 15 years old MCAT metric for each sampled site.

## Create Abundance
The purpose of this step is to calculate the Abundance MCAT metric for each sampled site.

* For the abundance metric, dissolve `Cordova_all_individuals` by `SAMPLE_ID` and `ENAME` (Scientific name).  Add 'SUM_NumberLive' field in the Statistics section of the tool. Uncheck create multipart features. Set Output Feature Class to the working .gdb and name it `Abundance`.

```{r}
# Import Abundance from geodatabase
abundance_sp <- arc2sp("//mvrdfs/EGIS/Work/EMP/HREP_Projects/SteamboatSlough/Mussels/SteamboatMusselModel/Data/SB_Mussels.gdb/SB_ILStatePlaneW/Abundance")

# Convert sp object to a data frame
abundance <- abundance_sp@data

# Remove OBJECTID field
abundance <- abundance[,-1]
```

```{r}
# Convert from num. of mussels per 0.25 sq m to num. per 1.0 square m
abundance$num_sq_m <- abundance$SUM_NumberLive * 4

# Calculate the quantiles
q <- quantile(abundance$num_sq_m)
q
```

```{r}
# Merge the abundance scores back onto the sp object
sample_abundance <- sp::merge(x = abundance_sp,
                              y = abundance,
                              by.x = "SampleID", by.y = "SampleID")
```

```{r eval=FALSE}
# Export to geodatabase
sp2arc(sample_abundance, fc_path = "//mvrdfs/EGIS/Work/EMP/HREP_Projects/SteamboatSlough/Mussels/SteamboatMusselModel/Data/SB_Mussels.gdb/SB_ILStatePlaneW/sample_abundance")
```


## Create Species Evenness
The purpose of this step is to calculate the Pielou's evenness index (range 0-1), estimated at the species level for each sampled site. 

* Create a table of SampleID by species by abundance (number live). Use the `Dissolve` tool:
    * Input Features: `Cordova_all_individuals`
    * Output feature class: `Cordova_sampleid_species`
    * Dissolve Fields: `SampleID`, `EName`
    * Statistics Fields: `NumberLive`, SUM
    * Create Multipart: unchecked

Ensure that the `Cordova_sampleid_species` feature class is of type `Point Features` and not `Multipoint Features`. If multipoint, then use the `Multipart to Singlepart` tool to convert from multipoint to point feature type. 

```{r}
# Import ArcGIS feature class into R
cordova_sampleid_species <- arc2sp("//mvrdfs/EGIS/Work/EMP/HREP_Projects/SteamboatSlough/Mussels/SteamboatMusselModel/Data/SB_Mussels.gdb/SB_ILStatePlaneW/Cordova_sampleid_species")

# Convert sp object to a data frame
sample_species <- cordova_sampleid_species@data

# Remove OBJECTID field
sample_species <- sample_species[,-1]

# Convert to vegan community data matrix-like format using labdsv::matrify
sample_species_matrix <- labdsv::matrify(sample_species)
```

```{r}
# Calculate the Shannon-Weaver diversity index
shannon_diversity <- vegan::diversity(sample_species_matrix, index = "shannon")

# Calculate the number species per site (see ?vegan::diversity)
species_number <- vegan::specnumber(sample_species_matrix)

# Use the following equation to calculate Pielou's evenness
# See the vegan Diversity Vignette for details
pielou_evenness <- shannon_diversity/log(species_number)

# Set NaN pielou_evenness values to zero
pielou_evenness <- ifelse(is.nan(pielou_evenness), 0, pielou_evenness)
# Create a data frame of results
pielou <- data.frame(sampleid = names(pielou_evenness), 
                     shannon_diversity, 
                     species_number, 
                     pielou_evenness)
```

```{r}
# Merge the Pielou scores back onto the sp object
sample_species_pielou <- sp::merge(x = cordova_sampleid_species,
                                   y = pielou,
                                   by.x = "SampleID", by.y = "sampleid")
```

```{r eval=FALSE}
# Export to geodatabase
sp2arc(sample_species_pielou, fc_path = "//mvrdfs/EGIS/Work/EMP/HREP_Projects/SteamboatSlough/Mussels/SteamboatMusselModel/Data/SB_Mussels.gdb/SB_ILStatePlaneW/sample_species_pielou")
```

* Create a table of SampleID by species Pielou evenness score. Use the `Dissolve` tool:
    * Input Features: `sample_species_pielou`
    * Output feature class: `species_pielou`
    * Dissolve Fields: `SampleID`
    * Statistics Fields: `shannon_diversity`, MEAN
                       `species_number`, MEAN
                       `pielou_evenness`, MEAN
    * Create Multipart: unchecked

* Add "SWD" fields.

## Create Tribe Eveness
The purpose of this step is to calculate the Pielou's evenness index (range 0-1), estimated at the tribe level.  (will probably need to add a new column for tribe; dissolve on this with sample ID)

* In the feature class `Cordova_all_individuals`, create a new text variable named `tribe`. 
* Use the `Field Calculator` tool on the `tribe` field to calculate its value using the following Python expression: `!Ename!.split()[0]`
* Create a table of SampleID by tribe by abundance (number live). Use the `Dissolve` tool:
    * Input Features: `Cordova_all_individuals`
    * Output feature class: `Cordova_sampleid_tribe`
    * Dissolve Fields: `SampleID`, `tribe`
    * Statistics Fields: `NumberLive`, SUM
    * Create Multipart: unchecked


```{r}
# Import ArcGIS feature class into R
cordova_sampleid_tribe <- arc2sp("//mvrdfs/EGIS/Work/EMP/HREP_Projects/SteamboatSlough/Mussels/SteamboatMusselModel/Data/SB_Mussels.gdb/SB_ILStatePlaneW/Cordova_sampleid_tribe")

# Convert sp object to a data frame
sample_tribe <- cordova_sampleid_tribe@data

# Remove OBJECTID field
sample_tribe <- sample_tribe[,-1]

# Convert to vegan community data matrix-like format using labdsv::matrify
sample_tribe_matrix <- labdsv::matrify(sample_tribe)
```


```{r}
# Calculate the Shannon-Weaver diversity index
tribe_shannon_diversity <- vegan::diversity(sample_tribe_matrix, index = "shannon")

# Calculate the number species per site (see ?vegan::diversity)
tribe_number <- vegan::specnumber(sample_tribe_matrix)

# Use the following equation to calculate Pielou's evenness
# See the vegan Diversity Vignette for details
tribe_pielou_evenness <- tribe_shannon_diversity/log(tribe_number)

# Set NaN pielou_evenness values to zero
tribe_pielou_evenness <- ifelse(is.nan(tribe_pielou_evenness), 0, 
                                tribe_pielou_evenness)
# Create a data frame of results
tribe_pielou <- data.frame(sampleid = names(tribe_pielou_evenness), 
                           tribe_shannon_diversity, 
                           tribe_number, 
                           tribe_pielou_evenness)
```

```{r}
# Merge the Pielou scores back onto the sp object
sample_tribe_pielou <- sp::merge(x = cordova_sampleid_tribe,
                                 y = tribe_pielou,
                                 by.x = "SampleID", by.y = "sampleid")
```

```{r eval=FALSE}
# Export to geodatabase
sp2arc(sample_tribe_pielou, fc_path = "//mvrdfs/EGIS/Work/EMP/HREP_Projects/SteamboatSlough/Mussels/SteamboatMusselModel/Data/SB_Mussels.gdb/SB_ILStatePlaneW/sample_tribe_pielou")
```

* Create a table of SampleID by tribe Pielou evenness score. Use the `Dissolve` tool:
    * Input Features: `sample_tribe_pielou`
    * Output feature class: `tribe_pielou`
    * Dissolve Fields: `SampleID`
    * Statistics Fields: `tribe_shannon_diversity`, MEAN
                       `tribe_number`, MEAN
                       `tribe_pielou_evenness`, MEAN
    * Create Multipart: unchecked

* Add "SWD" fields.

## Create ES 100
The purpose of this step is to calculate the ES 100 MCAT metric for each sampled site. ES_100 is the expected number of species with a sample size of 100 estimated by rarefaction based on random resampling of the data [@dunn2016]. 

ES_100-species richness estimated by rarefaction; caveat: sites need to be compared based on an equal sample size because # of species and # of individuals sampled are large correlated.  Not sure how best to approach this one; I defer to you!

```{r}
# Uses the sample_species_matrix calculated from the species evenness section

# Calculate rarefaction using a sample size of 100
rarefy_es_100 <- vegan::rarefy(x = sample_species_matrix, 
                               sample = 100)

# Calculate rarefaction using a reasonable sample size
rarefy_rowmedian <- vegan::rarefy(x = sample_species_matrix,
                                  sample = median(rowSums(sample_species_matrix)))

# Create a data frame of results
rarefy_species <- data.frame(sampleid = names(rarefy_es_100), 
                             rarefy_es_100,
                             rarefy_rowmedian,
                             shannon = diversity(sample_species_matrix),
                             specnumber = specnumber(sample_species_matrix))
```

```{r}
# Merge the rarefaction scores back onto the sp object
rarefy_sampleid_species <- sp::merge(x = cordova_sampleid_species,
                                     y = rarefy_species,
                                     by.x = "SampleID", by.y = "sampleid")
```

```{r eval=FALSE}
# Export to geodatabase
sp2arc(rarefy_sampleid_species, fc_path = "//mvrdfs/EGIS/Work/EMP/HREP_Projects/SteamboatSlough/Mussels/SteamboatMusselModel/Data/SB_Mussels.gdb/SB_ILStatePlaneW/rarefy_sampleid_species")
```

* Create a table of SampleID by tribe Pielou evenness score. Use the `Dissolve` tool:
    * Input Features: `rarefy_sampleid_species`
    * Output feature class: `rarefy_samples`
    * Dissolve Fields: `SampleID`
    * Statistics Fields: `SUM_NumberLive`, MEAN
                         `rarefy_es_100`, MEAN
                         `rarefy_rowmedian`, MEAN
                         `shannon`, MEAN
                         `specnumber`, MEAN
    * Create Multipart: unchecked

* Add "SWD" fields.


## Determine MCAT Metric per site threshold
The purpose of this section is to display the range of calculated values for each of the MCAT metrics for each of the sites in the Steamboat Island study area. The MCAT report is clear about metric thresholds to use for evaluating quality mussel beds. However, this study is using the individual site unit of analysis. Therefore, decisions must be made on what is the appropriate metric threshold to use for the site level of analysis. The descriptive statistics in this section will help to inform that decsion. 

```{r message=FALSE, warning=FALSE, include=FALSE}
### Import MCAT Metrics from geodatabase
## Percent Listed
percent_listed_sp <- arc2sp("//mvrdfs/EGIS/Work/EMP/HREP_Projects/SteamboatSlough/Mussels/SteamboatMusselModel/Data/SB_Mussels.gdb/SB_ILStatePlaneW/PercentListed")

# Convert sp object to a data frame
percent_listed <- percent_listed_sp@data

## Percent Tolerant
percent_tolerant_sp <- arc2sp("//mvrdfs/EGIS/Work/EMP/HREP_Projects/SteamboatSlough/Mussels/SteamboatMusselModel/Data/SB_Mussels.gdb/SB_ILStatePlaneW/PercentTolerant")

# Convert sp object to a data frame
percent_tolerant <- percent_tolerant_sp@data

## Percent Tribe Lampsilini
percent_lamp_sp <- arc2sp("//mvrdfs/EGIS/Work/EMP/HREP_Projects/SteamboatSlough/Mussels/SteamboatMusselModel/Data/SB_Mussels.gdb/SB_ILStatePlaneW/PercentLampsilini")

# Convert sp object to a data frame
percent_lamp <- percent_lamp_sp@data

## Percent Juveniles
percent_juv_sp <- arc2sp("//mvrdfs/EGIS/Work/EMP/HREP_Projects/SteamboatSlough/Mussels/SteamboatMusselModel/Data/SB_Mussels.gdb/SB_ILStatePlaneW/PercentJuvenile")

# Convert sp object to a data frame
percent_juv <- percent_juv_sp@data

## Percent over 15 years
percent_over15_sp <- arc2sp("//mvrdfs/EGIS/Work/EMP/HREP_Projects/SteamboatSlough/Mussels/SteamboatMusselModel/Data/SB_Mussels.gdb/SB_ILStatePlaneW/PercentOver15yrs")

# Convert sp object to a data frame
percent_over15 <- percent_over15_sp@data

## Abundance
abund_sp <- arc2sp("//mvrdfs/EGIS/Work/EMP/HREP_Projects/SteamboatSlough/Mussels/SteamboatMusselModel/Data/SB_Mussels.gdb/SB_ILStatePlaneW/sample_abundance")

# Convert sp object to a data frame
abund <- abund_sp@data

## Species Evenness
species_pielou_sp <- arc2sp("//mvrdfs/EGIS/Work/EMP/HREP_Projects/SteamboatSlough/Mussels/SteamboatMusselModel/Data/SB_Mussels.gdb/SB_ILStatePlaneW/species_pielou")

# Convert sp object to a data frame
species_pielou <- species_pielou_sp@data

## Tribe Evenness
tribe_pielou_sp <- arc2sp("//mvrdfs/EGIS/Work/EMP/HREP_Projects/SteamboatSlough/Mussels/SteamboatMusselModel/Data/SB_Mussels.gdb/SB_ILStatePlaneW/tribe_pielou")

# Convert sp object to a data frame
tribe_pielou <- tribe_pielou_sp@data

## ES_100
rarefy_samples_sp <- arc2sp("//mvrdfs/EGIS/Work/EMP/HREP_Projects/SteamboatSlough/Mussels/SteamboatMusselModel/Data/SB_Mussels.gdb/SB_ILStatePlaneW/rarefy_samples")

# Convert sp object to a data frame
rarefy_samples <- rarefy_samples_sp@data

```

### Percent Listed
```{r echo=FALSE, fig.height=3, fig.width=6.5}
percent_listed_p <- ggplot(percent_listed, aes(Perc_listed)) + 
  geom_histogram(binwidth = 5) + 
  geom_vline(xintercept = 3, color = "red", size = 1.5)
print(percent_listed_p)
```

### Percent Tolerant
```{r echo=FALSE, fig.height=3, fig.width=6.5}
percent_tolerant_p <- ggplot(percent_tolerant, aes(Perc_tolerant)) + 
  geom_histogram(binwidth = 5) + 
  geom_vline(xintercept = 40, color = "red", size = 1.5)
print(percent_tolerant_p)
```

### Percent Tribe Lampsilini
```{r echo=FALSE, fig.height=3, fig.width=6.5}
percent_lamp_p <- ggplot(percent_lamp, aes(perc_lampsilini)) + 
  geom_histogram(binwidth = 5) + 
  geom_vline(xintercept = 40, color = "red", size = 1.5)
print(percent_lamp_p)
```


### Percent Juveniles
```{r echo=FALSE, fig.height=3, fig.width=6.5}
percent_juv_p <- ggplot(percent_juv, aes(Perc_juvenile)) + 
  geom_histogram(binwidth = 5) + 
  geom_vline(xintercept = 50, color = "red", size = 1.5)
print(percent_juv_p)
```

### Percent over 15 years
```{r echo=FALSE, fig.height=3, fig.width=6.5}
percent_over15_p <- ggplot(percent_over15, aes(Perc_over_15yrs)) + 
  geom_histogram(binwidth = 5) + 
  geom_vline(xintercept = 5, color = "red", size = 1.5)
print(percent_over15_p)
```

### Abundance
```{r echo=FALSE, fig.height=3, fig.width=6.5}
abundance_p <- ggplot(abund, aes(num_sq_m)) + 
  geom_histogram(binwidth = 5) + 
  geom_vline(xintercept = 13, color = "red", size = 1.5)
print(abundance_p)
```

### Species Evenness
```{r echo=FALSE, fig.height=3, fig.width=6.5}
species_pielou_p <- ggplot(species_pielou, aes(MEAN_pielou_evenness)) + 
  geom_histogram(bins = 40) + 
  geom_vline(xintercept = 0.7, color = "red", size = 1.5)
print(species_pielou_p)
```

### Tribe Evenness
```{r echo=FALSE, fig.height=3, fig.width=6.5}
tribe_pielou_p <- ggplot(tribe_pielou, aes(MEAN_tribe_pielou_evenness)) + 
  geom_histogram(bins = 40) + 
  geom_vline(xintercept = 0.8, color = "red", size = 1.5)
print(tribe_pielou_p)
```

### ES_100
```{r echo=FALSE, fig.height=3, fig.width=6.5}
rarefy_samples_p <- ggplot(rarefy_samples, aes(MEAN_rarefy_es_100)) + 
  geom_histogram(binwidth = 1) + 
  geom_vline(xintercept = 3, color = "red", size = 1.5)
print(rarefy_samples_p)
```


##  Choice of Site Level MCAT Metric Thresholds
The discriptive statistics presented in the previous section were used to select MCAT metrics thresolds. 

```{r echo=FALSE}
metric_thresholds <- tribble(
~metrics,          ~ poor,            ~ fair,                      ~ good,        ~ site_thresh, ~no_sites,
"% listed"         , "<0.6"           , "0.6-3.6"                  , ">3.6"       , ">3"         , nrow(percent_listed[percent_listed$Perc_listed > 3, ]),
"% tolerant"       , ">62.7"          , "38.3-62.7"                , "<38.3"      , "<40"        , nrow(percent_tolerant[percent_tolerant$Perc_tolerant < 40, ]),
"% lampsilini"     , "<17.2 or >56.4" , "<17.2-34.7 or >39.5-56.4" , ">34.7-39.5" , ">40"        , nrow(percent_lamp[percent_lamp$perc_lampsilini > 40, ]),
"% juveniles"      , "<19.8"          , "<19.8-49.3"               , ">49.3"      , ">50"        , nrow(percent_juv[percent_juv$Perc_juvenile > 50, ]),
"% >= 15 years"    , "<0.8 or >16.0"  , ">5.6-16.0"                , ">2.4-5.6"   , ">5"         , nrow(percent_over15[percent_over15$Perc_over_15yrs > 5, ]),
"abundance"        , "<8"             , "8-13"                     , ">13"        , ">13"        , nrow(abund[abund$num_sq_m > 13, ]),
"species evenness" , "<0.665"         , "0.665-0.780"              , ">0.780"     , ">0.7"       , nrow(species_pielou[species_pielou$MEAN_pielou_evenness > 0.7, ]),
"tribe evenness"   , "<0.719"         , "0.719-0.823"              , ">0.823"     , ">0.8"       , nrow(tribe_pielou[tribe_pielou$MEAN_tribe_pielou_evenness > 0.8, ]),
"ES 100"           , "<11.5"          , "11.5-15.7"                , ">15.7"      , "3*"         , nrow(rarefy_samples[rarefy_samples$MEAN_rarefy_es_100 > 3, ])
)
```

```{r echo=FALSE, fig}
kable(metric_thresholds,
      col.names = c("Metrics", "Poor", "Fair", "Good", "Site Threshold", "No. of Sites"),
      caption = "Translating MCAT Metric Thresholds from the Bed to the Site Scale. ") %>%
  kable_styling("striped", full_width = FALSE) %>%
  add_header_above(c("", "Bed Scale" = 3, "Site Scale" = 2)) %>%
  footnote(symbol = c("The site threshold for this metric is lower than the bed threshold due to the lower number of individuals of each species found at each sampled site. "))
```


## Export to Maxent "SWD" format





<!--chapter:end:03-MusselMetrics.Rmd-->

# Create Backgound Data

## Create Background Data
Create a background to constrain points to aquatic areas (more data resolution) and reduce Maxent run times.

* Use the Create Random Points (Data Management) tool to create a background restricted to aquatic areas:

    * Output location: `\\mvrdfs\EGIS\Work\EMP\HREP_Projects\SteamboatSlough\Mussels\SteamboatMusselModel\Data\SB_Mussels.gdb\`
    * Output feature class: `background`
    * Constraining feature class:`aquatic_areas_5m`
    * Number of points: 10000
    * Mininum allowed distance: 25m
    * Leave all others as defaults
    * Click Ok to run the tool

* Open the `background` attribute table. Add 3 fields titled 'species' Type=string and Length=20; 'longitude' Type=double; and 'latitude' Type=double; in that order. Use field calculator to populate the field with species="background". Use Calculate Geometry to populate the 'longitude' and 'latitude' fields, ensuring that the coordinate system is set to 'NAD 1983 2011 US Feet' and 'StatePlane Illinois West FIPS 1202' and the units are set to US feet. Do the same for `unionidae_swd`.
* Use Extract Multi Values to Points (spatial analyst) tool to write new values (velocity, depth, slope, and ss) onto feature classes `unionidae_swd` and `background`.

     * Input point features: `\\mvrdfs\EGIS\Work\EMP\HREP_Projects\SteamboatSlough\Mussels\SteamboatMusselModel\Data\SB_Mussels.gdb\background`
     * Input rasters: velocity, depth, slope, and ss (use rasters in the `Adh.gdb`, not .bil)  copy unionidae_swd and delete 3 variable fields. Ensure all layers have same spatial reference before running the tool. Copy .gdb on local drive prior to running Maxent.
     
* Ensure all layers have same spatial reference before running the tool.  
* May have to run Project (Data Mgmt) tool on the `unionidae_swd` points feature class prior to running Multi Values to Points tool. 
* Export `background` and each mussel metric as .csv files. Ensure ObjectID field is deleted in Excel prior to upload in Maxent.











<!--chapter:end:04-Background.Rmd-->

# Prepare Environmental Predictor Data

## Create Mask
The goal of this step is to define no data areas outside of the study.

* Create a new polygon feature class named: `\\mvrdfs\EGIS\Work\EMP\HREP_Projects\SteamboatSlough\Mussels\SteamboatMusselModel\Data\SB_Mussels.gdb\SB_ILStatePlaneW\study_area`.
* Edit the the 'study_area' feature class by tracing one of the rasters created in the previous step (e.g., 'velocity'). The resulting polygon will become the clip feature for the next tool run.
* Use the `Aquatic_Areas_2010` layer to prepare data for the Clip (Analysis) tool (`\\mvrdfs\egis\Data\Layers\Biology\Aquatic_Areas_2010.lyr`).

    * Input Features: `Aquatic Areas 2010\Classifications\Aquatic Areas: Level 2`
    * Clip Features: `study_area`
    * Output Feature Class: `\\mvrdfs\EGIS\Work\EMP\HREP_Projects\SteamboatSlough\Mussels\SteamboatMusselModel\Data\SB_Mussels.gdb\aquatic_areas`
    
* Delete "Isolated Floodplain Lake" records from the `aquatic_areas` attribute table.  
* Use the Dissolve (Data Management tool) `aquatic_areas` using 'LAND_WATER' as the dissolve field. 

    * Output feature class: `\\mvrdfs\EGIS\Work\EMP\HREP_Projects\SteamboatSlough\Mussels\SteamboatMusselModel\Data\SB_Mussels.gdb\aquatic_areas_dissolve`
    * Create multipart features: uncheck box  
    
* Open an edit session to edit the `aquatic_areas_dissolve` attribute table. Select the row where 'LAND_WATER' field is set to Land, and delete.
* Run the Multipart to Singlepart Tool (Data Management) on `aquatic_areas_dissolve`. Name the feature class `aquatic_areas_dissolve_singlepart`.
* Turn on `topobathy` and use it as a guide to edit out areas where where it is too shallow in the `aquatic_areas_dissolve_singlepart` feature class. Be sure to merge the polygons before saving edits and delete intermediates `aquatic_areas_dissolve` and `aquatic_areas_dissolve_singlepart`.
* Use the Buffer (Analysis) tool to expand  the study area beyond the extent of valid values. This expanded area will be used to define no data portions of the modeled Adh values.

    * Input features: `aquatic_areas`
    * Output Feature Class: `\\mvrdfs\egis\Work\EMP\HREP_Projects\SteamboatSlough\Mussels\SteamboatMusselModel\Data\SB_Mussels.gdb\SB_ILStatePlaneW\aquatic_areas_20m`
    * Distance: 20 meters
    
* Use the Polygon to Raster (Conversion) tool to convert the `aquatic_areas_20m` feature class to a raster.

    * Input Features: `aquatic_areas_20m`
    * Value field: `OBJECTID`
    * Output Raster Dataset: `\\mvrdfs\egis\Work\EMP\HREP_Projects\SteamboatSlough\Mussels\SteamboatMusselModel\Adh\Existing_condition\existingcondition.gdb\mask`
* Set Environments

    * Ouput Coordinates: `NAD_1983_2011_StatePlane_Illinois_West_FIPS_1202_Ft_US`
    * Processing Extent: `study_area`
    * Cell Size: 3 ft

## Set Environments
The purpose of this step is to set the Environments to `mask` so that all the subsequent Adh raster outputs will have the same coordinate system, extent, cell alignment, cell size, and no data area.
*Set Environments

    * Ouput Coordinates: `mask`
    * Processing Extent: `mask`
    * Snap raster: `mask`
    * Cell Size: `mask`
    * Mask: `mask`

## Process ADH Model Outputs
The goal of this step is to convert the raw Adaptive Hydraulics Model (Adh) data (depth, slope velocity, sheer stress, froude_number, reynolds_number) into raster format that can be input to the Maxent model. Adh models will be produced for the Q5 (high flow) and Q95 (low flow) flow conditions. This wil result in the creation of 12 raster (one for each of 6 Adh parameter) by 2 flow conditions. 

* Create a new folder and geodatabase to represent the alternative that this Adh model represents. In this example we will be modeling the existing condition alternative: `\\mvrdfs\egis\Work\EMP\HREP_Projects\SteamboatSlough\Mussels\SteamboatMusselModel\Adh\Existing_condition/existingcondition.gdb`
* Get the Adh output from the hydraulic engineer. Copy it to the alternative folder you created in the last step. EC-HH Working directory: `\\mvr-netapp2\MVRDATA\ED\ec-hh\Mississippi_River\Steamboat Island\AdH\Model Runs\`
* Use Excel to convert the space-delimited `.txt` file to `.csv`. Open a blank Excel workbook and open the `.txt` file. Use the import wizard to convert the space-delimited file into an Excel workbook. Ensure the import works correctly, delete any header information above the row of field names, and then save as a `.csv` file. 
* Import the `.csv` file into the `existingcondition` geodatabase. Name the table by the flow condition it represents (Q5, Q95). 
* Add the Q5 table into a blank ArcMap document. 
* Right-click the `Q5` table and click "Display XY Data".
* Set appropriate 'X','Y', and 'Z' fields.
* Identify the datum and coordinate system recorded in the Adh table; all future output feature classes and rasters will need to be in the same projected coordinate system. In this example, Datum: 'NAD 1983 2011 US Feet' and Coordinate System: 'StatePlane Illinois West FIPS 1202'.
* Export `Q5_Events` as new point feature class named `Q5_pts`.
* Use the IDW (Spatial Analyst) tool to interpolate a raster surface.

    * Input point features: `Q5_pts`. 
    * Z value field: velocity
    * Output raster: `\\mvrdfs\EGIS\Work\EMP\HREP_Projects\SteamboatSlough\Mussels\SteamboatMusselModel\Adh\Existing_condition\existingcondition.gdb\q5_velocity`
    
* Complete these steps for the other variables, 'depth', 'slope', and 'sheer stress', 'froude_number', 'reynolds_number'.

## Convert to .bil format
The goal of this step is to convert the Adh output into a format that can be used by Maxent. The .bil format is one of the formats common to both Maxent and Esri. This step will repeated for all Adh (Q5, Q95) predictors.

* Use the Copy Raster (Data Management) tool to covert the Adh predictor rasters into .bil format.

    * Input Raster: `q5_velocity`
    * Output Raster Dataset: `\\mvrdfs\egis\Work\EMP\HREP_Projects\SteamboatSlough\Mussels\SteamboatMusselModel\Adh\Existing_condition\q5_velocity.bil` (Note: cannot save .bil format within the .gdb)
    * Format: Esri BIL


## Calculate Slope
The goal of the step is to calculate slope from the Adh depth variable created in the previous step.

* Use the Slope (Spatial Analyst) tool to calculate slope.

    * Input raster: `q5_depth`
    * Output raster: `\\mvrdfs\egis\Work\EMP\HREP_Projects\SteamboatSlough\Mussels\SteamboatMusselModel\Adh\Existing_condition\existingcondition.gdb\q5_slope`
    * Output measurement: degree
    * Z factor = 1 (linear units same)
    
* Use Copy Raster (Data Management) tool to convert the `q5_slope` raster into .bil format, as outlined in previous step.
  

<!--chapter:end:05-AdhPrep.Rmd-->

# Prepare Wind and Wave Data

## Identify Appropriate Weather Stations

## Obtain Wind Data

## Determine Period of Record



<!--chapter:end:06-WindWaveModels.Rmd-->

# Develop Maxent Mussel Model

## Convert from `.bil` to `.mxe` format
To speed model creation (from 10's of hours to 10's of minutes), convert the predictors to the Maxent `.mxe` format.

* Open command prompt and change directory by typing `c:` and then `cd C:\Users\b6pdpdem\Documents\SteamboatMusselModel`
* Create predictor_mxe folder

```java -Xmx5g -cp maxent.jar density.Convert C:\Users\b6pdpdem\Documents\SteamboatMusselModel\predictors bil C:\Users\b6pdpdem\Documents\SteamboatMusselModel\predictors_mxe mxe```


## Run Maxent


## Convert model output from 
To convert the `.mxe` format back to a format that can be read by ESRI. 

```java -Xmx5g -cp maxent.jar density.Convert C:\Users\b6pdpdem\Documents\SteamboatMusselModel\sb_existing_20181203_2 mxe C:\Users\b6pdpdem\Documents\SteamboatMusselModel\sb_existing_20181203_2 bil```

* Use the `Define Projection` tool to set the projection of the `.bil` rasters. 

## Maxent speed Issues
Follow these simple performance guidelines to reduce Maxent run time from a week to half an hour. 

* Run the model on a physical computer, not on a virtual machine. 
* Select a computer with the fastest processor and most memory available. 
* Increase the memory available to the java virtual machine (JVM). Edit the first line of the `maxent.bat` file to something like the following: `java -Xmx5g -jar maxent.jar`, where the `-Xmx` parameter sets the maximum heap size for the JVM. Set it to something less than the total amount of physical memory on your computer. 
* Do not run the model across the LAN (inputs and outputs stored on a network file system). Copy all inputs and write all outputs to the local file system.
* Use a solid state drive for storing inputs and outputs. 
* Use the "SWD" format for specifying both samples (species occurances) and environmental layers (background points). 
* Convert all predictor grids to the Maxent `.mxe` format. 
* Write all outputs to the Maxent `.mxe` format. 
* Set the `threads` parameter to at least the number of cores on your computer (or a large percentage of logical processors) to speed processing of replicates. 







<!--chapter:end:07-MaxentSetup.Rmd-->

# Model Interpretation

## ?



<!--chapter:end:08-Interpretation.Rmd-->

